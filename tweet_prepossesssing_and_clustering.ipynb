{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1182eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e362d1ca-0b10-49e5-9611-ba2150f2bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d27b13",
   "metadata": {},
   "source": [
    "# Removing non-english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e09bb41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = 'unknown'\n",
    "    return lang\n",
    "def is_english(text):\n",
    "    english_pattern = re.compile(r'^[a-zA-Z0-9!@#$%^&*()_+=\\-[\\]{}|;:\",.<>?`~\\s]+$')\n",
    "    return bool(english_pattern.findall(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7fbb4",
   "metadata": {},
   "source": [
    "# Hashtag Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c2a9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtag(df):\n",
    "    df['Hashtag'] = df['Text'].apply(\n",
    "        lambda x: [hashtag.lower() for hashtag in re.findall(r\"#(\\w+)\", x)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8bdfb",
   "metadata": {},
   "source": [
    "# Tokenization and POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a482433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lemmatize function\n",
    "def lemmatize(word_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in word_list]\n",
    "\n",
    "# Defining the tokenize function\n",
    "def tokenize(df):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Ensures all rows in 'Text' are strings, replacing NaN or invalid values\n",
    "    df['Text'] = df['Text'].fillna('').astype(str)\n",
    "    # Tokenization of the tweets\n",
    "    df['Tokenized_Tweets'] = df['Text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    # Lemmatization of the tokens\n",
    "    df['Tokenized_Tweets'] = df['Tokenized_Tweets'].apply(lambda x: lemmatize(x))\n",
    "    # Performing POS tagging on tokenised text\n",
    "    df['Tokenized_Tweets'] = df['Tokenized_Tweets'].apply(lambda x: pos_tag(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675afc3f",
   "metadata": {},
   "source": [
    "# Stop words,punctuation,special word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5857cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(pos_tagged_tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token,pos in pos_tagged_tokens if token.lower() not in stop_words and  token.isalpha()]\n",
    "    filtered_tokens = [token.lower() for token in filtered_tokens if token not in string.punctuation]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f4f29",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b5de5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(sentences):\n",
    "    dictonary = {}\n",
    "    for sentence in sentences:\n",
    "        for word in  sentence:\n",
    "            dictonary[word] = dictonary.get(word,0)+1\n",
    "    return dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e48ed79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dict(bag_of_words,tweets_list):\n",
    "    doc_count={}\n",
    "    for word in bag_of_words:\n",
    "        for tweet in tweets_list:\n",
    "            if word in tweet:\n",
    "                doc_count[word] = doc_count.get(word,0)+1\n",
    "    return doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce8c7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_vector(tweets_list, bag_of_words):\n",
    "    N = len(tweets_list)\n",
    "    doc_count = count_dict(bag_of_words, tweets_list)\n",
    "    result_list = []\n",
    "    for tweet in tweets_list:\n",
    "        line = []\n",
    "        tf_vector = Counter(tweet)\n",
    "        for word in bag_of_words:\n",
    "            tf = tf_vector.get(word, 0)\n",
    "            idf = np.log(N / (1 + doc_count.get(word,0)))\n",
    "            line.append(tf * idf)\n",
    "        result_list.append(line)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f5e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_vector(tweets_list,bag_of_words):\n",
    "    matrix = [[0]* len(bag_of_words) for i in range(len(tweets_list))]\n",
    "    for i in range(len(tweets_list)):\n",
    "        word_set = set(tweets_list[i])\n",
    "        for j,word in enumerate(bag_of_words):\n",
    "            if word in word_set:\n",
    "                matrix[i][j] = 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f9b07",
   "metadata": {},
   "source": [
    "# Similarity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92ce2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(array1, array2):\n",
    "    dot_product = np.dot(array1, array2)\n",
    "    magnitude1 = np.linalg.norm(array1)\n",
    "    magnitude2 = np.linalg.norm(array2)\n",
    "    \n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81714e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1,list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    return len(set1 & set2)/len(set1 | set2 )\n",
    "def jaccard_similarity_matrix(vector):\n",
    "    matrix = []\n",
    "    for list1 in vector:\n",
    "        temp = []\n",
    "        for list2 in vector:\n",
    "            temp.append(jaccard_similarity(list1,list2))\n",
    "        matrix.append(temp)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98f830f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_similarity(list1,list2):\n",
    "    common_count = 0\n",
    "    for i in range(len(list1)):\n",
    "        if list1[i] == list2[i]:\n",
    "            common_count+=1\n",
    "    return common_count/(2*len(list1))\n",
    "def dice_similarity_matrix(vector):\n",
    "    matrix = []\n",
    "    for list1 in vector:\n",
    "        temp = []\n",
    "        for list2 in vector:\n",
    "            temp.append(dice_similarity(list1,list2))\n",
    "        matrix.append(temp)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a5095",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1870f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(vectors_list, K, max_iters=100):\n",
    "    vectors = np.array(vectors_list)\n",
    "    centroids = vectors[np.random.choice(vectors.shape[0], K, replace=False)]\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        similarity_matrix = cosine_similarity(centroids, vectors)\n",
    "        cluster_assignments = np.argmax(similarity_matrix, axis=0)\n",
    "        new_centroids = np.array([vectors[cluster_assignments == k].mean(axis=0) if np.sum(cluster_assignments == k) > 0 else vectors[np.random.choice(vectors.shape[0])] for k in range(K)])\n",
    "        if np.array_equal(centroids, new_centroids):\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc34d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "def dbscan(data, eps, min_samples):\n",
    "    def range_query(data, point, eps):\n",
    "        neighbors = []\n",
    "        for i, neighbor in enumerate(data):\n",
    "            if euclidean_distance(point, neighbor) <= eps:\n",
    "                neighbors.append(i)\n",
    "        return neighbors\n",
    "\n",
    "    def dbscan_recursive(data, point_index, cluster_id):\n",
    "        neighbors = range_query(data, data[point_index], eps)\n",
    "        if len(neighbors) < min_samples:\n",
    "            labels[point_index] = -1  # Mark as noise\n",
    "        else:\n",
    "            labels[point_index] = cluster_id\n",
    "            for neighbor_index in neighbors:\n",
    "                if labels[neighbor_index] == 0:\n",
    "                    dbscan_recursive(data, neighbor_index, cluster_id)\n",
    "\n",
    "    num_points, _ = data.shape\n",
    "    labels = np.zeros(num_points)\n",
    "\n",
    "    cluster_id = 0\n",
    "    for point_index in range(num_points):\n",
    "        if labels[point_index] == 0:\n",
    "            cluster_id += 1\n",
    "            dbscan_recursive(data, point_index, cluster_id)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afc3cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_clustering(vectors, threshold=0.5):\n",
    "    assert all(isinstance(vector, np.ndarray) and vector.ndim == 1 for vector in vectors), \"All vectors must be 1D arrays\"\n",
    "    clusters = [] \n",
    "    cluster_assignments = []  \n",
    "\n",
    "    for vector in vectors:\n",
    "        vector_added = False \n",
    "\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            centroid = np.mean(cluster, axis=0) if cluster else np.zeros_like(vector)\n",
    "            similarity = cosine_similarity(vector, centroid)\n",
    "\n",
    "            if similarity > threshold:\n",
    "                clusters[i].append(vector)\n",
    "                cluster_assignments.append(i) \n",
    "                vector_added = True\n",
    "                break\n",
    "        if not vector_added:\n",
    "            clusters.append([vector])\n",
    "            cluster_assignments.append(len(clusters) - 1) \n",
    "\n",
    "    return cluster_assignments, len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2baba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_words(cluster_texts,k):\n",
    "    text_lists = [[] for _ in range(k)]\n",
    "    cluster_sets = []\n",
    "    for i,cluster_tweets in enumerate(cluster_texts):\n",
    "        for tweet in cluster_tweets:\n",
    "            text_lists[i].extend(tweet)\n",
    "    for i in range(k):\n",
    "        set = Counter(text_lists[i])\n",
    "        cluster_sets.append(set)\n",
    "    return cluster_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "252d8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(word_freq_dict, cluster_number, save_path):\n",
    "    if len(word_freq_dict) == 0:\n",
    "        return\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for Cluster {cluster_number}')\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b870ea6-83d9-47d7-9c82-86bf2d3a930f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db1c42-be2b-4b8c-a120-f60e4350dc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35f6cc-1f24-4a97-84e1-063281889b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
