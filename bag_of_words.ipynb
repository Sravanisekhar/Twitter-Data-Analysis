{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd92d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from tweet_prepossesssing_and_clustering import is_english,extract_hashtag,remove_stop_words,lemmatize,tokenize,generate_bow,count_dict,tf_idf_vector,one_hot_encoding_vector,cosine_similarity,jaccard_similarity,dice_similarity,k_means_clustering,most_frequent_words,generate_word_cloud,iterative_clustering\n",
    "from chainning_and_analysis import calculate_silhouette_scores,calculate_cluster_weights,event_clusters_filter,generate_clusters,write_clusters_to_text_and_hashtags,read_clusters_from_file,read_hashtags_from_file,centroid,calculate_similarity,textual_similarity,create_bipartite_graph,apply_hungarian_method,create_cluster_chains,get_folder_names,write_cluster_chains,analyze_sentiment,plot_pie_chart,generate_emoji_image,generate_word_cloud,analyze_and_visualize_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19811b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import emoji\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import operator\n",
    "from textblob import TextBlob\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import emojis\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962a7498-9b69-4122-82e3-de081f8868d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('omw-1.4')  # Optional, for WordNet lemmatizer's additional language support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b428e3-b101-4522-8e72-4d2d87bcfca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng', download_dir='C:\\\\nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c66b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files(input_folder):\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder '{input_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            df = pd.read_csv(file_path, names=['Text'])\n",
    "            df['Text'] = df['Text'].fillna('').astype(str)\n",
    "            df = df[df['Text'].apply(is_english)]\n",
    "\n",
    "            extract_hashtag(df)\n",
    "            tokenize(df)\n",
    "            df['filtered_tokens'] = df['Tokenized_Tweets'].apply(lambda x: remove_stop_words(x) if isinstance(x, list) else [])\n",
    "            df = df[df['filtered_tokens'].apply(lambda x: len(x) > 0)]\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Generate Bag of Words\n",
    "            vectorizer = CountVectorizer()\n",
    "            text_data = [\" \".join(tokens) for tokens in df['filtered_tokens']]\n",
    "            bow_matrix = vectorizer.fit_transform(text_data).toarray()\n",
    "\n",
    "            cluster, no_of_clusters = iterative_clustering(bow_matrix, 0.1)\n",
    "            cluster_qualities = calculate_silhouette_scores(bow_matrix, np.array(cluster))\n",
    "            cluster_weights = calculate_cluster_weights(bow_matrix, cluster)\n",
    "            event_clusters = event_clusters_filter(cluster_qualities, cluster_weights, 0.7, 6.0)\n",
    "\n",
    "            if len(event_clusters) < 1:\n",
    "                print(f\"No valid clusters found in file: {filename}\")\n",
    "                continue\n",
    "\n",
    "            event_clusters_tweets, event_clusters_hashtags = generate_clusters(\n",
    "                list(df['filtered_tokens']), cluster, event_clusters, list(df['Hashtag'])\n",
    "            )\n",
    "\n",
    "            folder_name = os.path.splitext(filename)[0]\n",
    "            output_folder = os.path.join('event_tweets_bow', folder_name)\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "            write_clusters_to_text_and_hashtags(event_clusters_tweets, event_clusters_hashtags,\n",
    "                                                os.path.join(output_folder, 'event_clusters_bow.txt'),\n",
    "                                                os.path.join(output_folder, 'event_hashtags_bow.txt'))\n",
    "\n",
    "            for i in range(len(event_clusters)):\n",
    "                generate_word_cloud(word_sets[i], i, os.path.join(output_folder, f\"img{i}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d05909-0913-4a7f-b048-ae1621cc333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: batch_1.txt\n",
      "No valid clusters found in file: batch_1.txt\n",
      "Processing file: batch_10.txt\n",
      "No valid clusters found in file: batch_10.txt\n",
      "Processing file: batch_11.txt\n",
      "No valid clusters found in file: batch_11.txt\n",
      "Processing file: batch_12.txt\n",
      "No valid clusters found in file: batch_12.txt\n",
      "Processing file: batch_13.txt\n",
      "No valid clusters found in file: batch_13.txt\n",
      "Processing file: batch_14.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: invalid value encountered in scalar divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid clusters found in file: batch_14.txt\n",
      "Processing file: batch_15.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: invalid value encountered in scalar divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid clusters found in file: batch_15.txt\n",
      "Processing file: batch_16.txt\n",
      "No valid clusters found in file: batch_16.txt\n",
      "Processing file: batch_17.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: invalid value encountered in scalar divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid clusters found in file: batch_17.txt\n",
      "Processing file: batch_18.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_sets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_text_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_tweets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 49\u001b[0m, in \u001b[0;36mprocess_text_files\u001b[1;34m(input_folder)\u001b[0m\n\u001b[0;32m     44\u001b[0m write_clusters_to_text_and_hashtags(event_clusters_tweets, event_clusters_hashtags,\n\u001b[0;32m     45\u001b[0m                                     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_clusters_bow.txt\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     46\u001b[0m                                     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_hashtags_bow.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(event_clusters)):\n\u001b[1;32m---> 49\u001b[0m     generate_word_cloud(\u001b[43mword_sets\u001b[49m[i], i, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_sets' is not defined"
     ]
    }
   ],
   "source": [
    "process_text_files('output_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31fbcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(),'event_tweets_bow')\n",
    "timestamps = get_folder_names(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "86451781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_1 and batch_14\n",
      "batch_14 and batch_15\n",
      "batch_15 and batch_16\n",
      "batch_16 and batch_17\n",
      "batch_17 and batch_18\n",
      "batch_18 and batch_19\n",
      "batch_19 and batch_20\n",
      "batch_20 and batch_3\n",
      "batch_3 and batch_5\n",
      "batch_5 and batch_6\n",
      "batch_6 and batch_7\n",
      "batch_7 and batch_8\n"
     ]
    }
   ],
   "source": [
    "resulting_chains = create_cluster_chains(timestamps,folder_path,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5675aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_dict(folder_path):\n",
    "    clusters_dict = {}\n",
    "    folders = sorted(os.listdir(folder_path))\n",
    "    for i in range(len(folders)-1):\n",
    "        current_folder = folders[i]\n",
    "        next_folder = folders[i+1]\n",
    "\n",
    "        current_file_path = os.path.join(folder_path, current_folder, 'event_clusters_bow.txt')\n",
    "        next_file_path = os.path.join(folder_path, next_folder, 'event_clusters_bow.txt')\n",
    "\n",
    "        current_clusters = read_clusters_from_file(current_file_path)\n",
    "        next_clusters = read_clusters_from_file(next_file_path)\n",
    "\n",
    "        clusters_dict[current_folder] = current_clusters\n",
    "        clusters_dict[next_folder] = next_clusters\n",
    "\n",
    "    return clusters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de3ff977",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = create_clusters_dict(os.path.join(os.getcwd(),'event_tweets_bow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "614cd095",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cluster_chains(resulting_chains,os.getcwd(),clusters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb638395",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Prabha\\\\Desktop\\\\mini project\\\\Twitter-Data-Analysis-main\\\\neutral\\\\emoji.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cluster_chains_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_chains\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment_analysis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43manalyze_and_visualize_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_chains_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:31\u001b[0m, in \u001b[0;36manalyze_and_visualize_clusters\u001b[1;34m(cluster_chains_folder, output_folder)\u001b[0m\n",
      "File \u001b[1;32m<string>:11\u001b[0m, in \u001b[0;36mgenerate_emoji_image\u001b[1;34m(sentiments, save_path)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Prabha\\\\Desktop\\\\mini project\\\\Twitter-Data-Analysis-main\\\\neutral\\\\emoji.png'"
     ]
    }
   ],
   "source": [
    "cluster_chains_folder = os.path.join(os.getcwd(),'cluster_chains_bow')\n",
    "output_folder = os.path.join(os.getcwd(),'Sentiment_analysis_bow')\n",
    "analyze_and_visualize_clusters(cluster_chains_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308befa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
