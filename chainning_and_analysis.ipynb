{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d725d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import emoji\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import operator\n",
    "from textblob import TextBlob\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import emojis\n",
    "from PIL import Image, ImageDraw, ImageFont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c06bbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from tweet_prepossesssing_and_clustering import is_english,extract_hashtag,remove_stop_words,lemmatize,tokenize,generate_bow,count_dict,tf_idf_vector,one_hot_encoding_vector,cosine_similarity,jaccard_similarity,dice_similarity,k_means_clustering,most_frequent_words,generate_word_cloud,iterative_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb00be7",
   "metadata": {},
   "source": [
    "# Cluster filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93e88379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_silhouette_scores(tfidf_vectors, cluster_assignments):\n",
    "    n = len(tfidf_vectors)\n",
    "    cluster_labels = sorted(set(cluster_assignments))\n",
    "    silhouette_scores = []\n",
    "    for current_cluster in cluster_labels:\n",
    "        cluster_indices = [i for i in range(n) if cluster_assignments[i] == current_cluster]\n",
    "        centroid = np.mean(tfidf_vectors[cluster_indices], axis=0)\n",
    "        a_values = [pairwise_distances(tfidf_vectors[i].reshape(1, -1), centroid.reshape(1, -1))[0][0]\n",
    "                    for i in cluster_indices]\n",
    "        a_avg = np.mean(a_values) if a_values else 0\n",
    "        b_values = []\n",
    "        for other_cluster in cluster_labels:\n",
    "            if other_cluster != current_cluster:\n",
    "                other_cluster_indices = [i for i in range(n) if cluster_assignments[i] == other_cluster]\n",
    "                other_cluster_centroid = np.mean(tfidf_vectors[other_cluster_indices], axis=0)\n",
    "                for i in cluster_indices:\n",
    "                    b_values.append(pairwise_distances(tfidf_vectors[i].reshape(1, -1), other_cluster_centroid.reshape(1, -1))[0][0])\n",
    "        b_avg = np.mean(b_values) if b_values else 0\n",
    "        silhouette_score_cluster = (b_avg - a_avg) / max(a_avg, b_avg) if max(a_avg, b_avg) > 0 else 0\n",
    "        silhouette_scores.append(silhouette_score_cluster)\n",
    "\n",
    "    return silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e09dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_weights(tfidf_vectors, cluster_assignments):\n",
    "    unique_clusters = np.unique(cluster_assignments)\n",
    "    cluster_weights = []\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_assignments == cluster)[0]\n",
    "        cluster_tfidf = np.sum(tfidf_vectors[cluster_indices], axis=0)\n",
    "        weight = np.sum(cluster_tfidf) / max(np.count_nonzero(cluster_tfidf), 1)\n",
    "        cluster_weights.append(weight)\n",
    "    return cluster_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9e92882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_clusters_filter(cluster_quality,cluster_weights,quality_threshold = 0.1,weight_threshold = 4.0):\n",
    "    alpha = 0.5\n",
    "    beta = 0.5\n",
    "    value = alpha * quality_threshold + beta * weight_threshold\n",
    "    event_clusters = []\n",
    "    for i in range(len(cluster_quality)):\n",
    "        if cluster_quality[i] * alpha + cluster_weights[i] * beta >= value:\n",
    "            event_clusters.append(i)\n",
    "    return event_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4b5229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clusters(tweet_list, cluster_assignment, event_clusters, hashtag_lists):\n",
    "    cluster_list = [[] for _ in range(len(set(cluster_assignment)))]\n",
    "    hashtag_counts_list = [{} for _ in range(len(set(cluster_assignment)))]\n",
    "\n",
    "    for i, (cluster, hashtags) in enumerate(zip(cluster_assignment, hashtag_lists)):\n",
    "        if cluster in event_clusters:\n",
    "            cluster_list[cluster].append(tweet_list[i])\n",
    "            for hashtag in hashtags:\n",
    "                hashtag_counts_list[cluster][hashtag] = hashtag_counts_list[cluster].get(hashtag, 0) + 1\n",
    "    cluster_list = [cluster for cluster in cluster_list if len(cluster) > 0]\n",
    "    hashtag_counts_list = [hashtags for hashtags in hashtag_counts_list if len(hashtags) > 0]\n",
    "    return cluster_list, hashtag_counts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31279c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_clusters_to_text_and_hashtags(event_clusters, hashtags_dicts, text_file_path, hashtags_file_path):\n",
    "    with open(text_file_path, 'w', encoding='utf-8') as text_file, open(hashtags_file_path, 'w', encoding='utf-8') as hashtags_file:\n",
    "        for cluster_index, (cluster, hashtags_dict) in enumerate(zip(event_clusters, hashtags_dicts)):\n",
    "            # Write tweets to the text file\n",
    "            text_file.write(f\"Cluster {cluster_index + 1}:\\n\")\n",
    "            for tweet_list in cluster:\n",
    "                for tweet in tweet_list:\n",
    "                    text_file.write(f\"{tweet}\\n\")\n",
    "                text_file.write(\"\\n\")\n",
    "\n",
    "            hashtags_file.write(f\"Cluster {cluster_index + 1} Hashtags:\\n\")\n",
    "            for hashtag, frequency in hashtags_dict.items():\n",
    "                hashtags_file.write(f\"{hashtag}: {frequency}\\n\")\n",
    "            \n",
    "            text_file.write(\"\\n\\n\")\n",
    "            hashtags_file.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6658a9",
   "metadata": {},
   "source": [
    "# Cluster chainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca99040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clusters_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    clusters = []\n",
    "    current_cluster = []\n",
    "    current_tweet = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Cluster\"):\n",
    "            if current_cluster:\n",
    "                clusters.append(current_cluster)\n",
    "            current_cluster = []\n",
    "        elif not line:\n",
    "            if current_tweet:\n",
    "                current_cluster.append(current_tweet)\n",
    "                current_tweet = []\n",
    "        else:\n",
    "            current_tweet.append(line)\n",
    "\n",
    "    if current_cluster:\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce083f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hashtags_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    hashtags_list = defaultdict(list)\n",
    "    current_cluster_no = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Cluster\"):\n",
    "            current_cluster_no = int(line.split()[1])\n",
    "        elif line:\n",
    "            hashtags_list[current_cluster_no].extend(line.split())\n",
    "    hashtags_dict_list = [{tag: hashtags_list[cluster].count(tag) for tag in set(hashtags_list[cluster])}\n",
    "                          for cluster in hashtags_list]\n",
    "\n",
    "    return hashtags_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0930e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(cluster,bow):\n",
    "    tf_idf_matrix = np.array(tf_idf_vector(cluster,bow))\n",
    "    return np.mean(tf_idf_matrix, axis=0)\n",
    "def calculate_similarity(cluster1, cluster2):\n",
    "    merged_cluster = cluster1\n",
    "    merged_cluster.extend(cluster2)\n",
    "    bow = generate_bow(merged_cluster)\n",
    "    centroid1 = centroid(cluster1,bow)\n",
    "    centroid2 = centroid(cluster2,bow)\n",
    "    return cosine_similarity(centroid1,centroid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45170c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_similarity(cluster1,cluster2):\n",
    "    text1 = [item for tweet in cluster1 for item in tweet]\n",
    "    text2 = [item for tweet in cluster2 for item in tweet]\n",
    "    return jaccard_similarity(text1,text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "656a9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bipartite_graph(current_clusters, current_hashtags, next_clusters, next_hashtags, threshold):\n",
    "    similarity_matrix = np.zeros((len(current_clusters), len(next_clusters)))\n",
    "\n",
    "    for i, current_cluster in enumerate(current_clusters):\n",
    "        current_hashtag = current_hashtags[i]\n",
    "\n",
    "        for j, next_cluster in enumerate(next_clusters):\n",
    "            next_hashtag = next_hashtags[j]\n",
    "\n",
    "            syntactic_similarity = calculate_similarity(current_cluster, next_cluster)\n",
    "            semantic_similarity = textual_similarity(current_cluster, next_cluster)\n",
    "            union_dict = {key: current_hashtag.get(key, 0) + next_hashtag.get(key, 0) for key in\n",
    "                          set(current_hashtag) | set(next_hashtag)}\n",
    "            intersection_dict = {key: min(current_hashtag.get(key, 0), next_hashtag.get(key, 0)) for key in\n",
    "                                 set(current_hashtag) & set(next_hashtag)}\n",
    "            hashtag_similarity = sum(intersection_dict.values()) / sum(union_dict.values())\n",
    "            overall_similarity = 0.4 * syntactic_similarity + 0.4 * semantic_similarity + 0.2 * hashtag_similarity\n",
    "            similarity_matrix[i, j] = overall_similarity\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4373478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hungarian_method(similarity_matrix, num_clusters_first, num_clusters_second):\n",
    "    optimal_assignment = {}\n",
    "    graph = np.array(similarity_matrix)\n",
    "    num_rows, num_cols = graph.shape\n",
    "    \n",
    "    for col in range(num_cols):\n",
    "        sorted_indices = np.argsort(graph[:, col])[::-1]\n",
    "        \n",
    "        for i in sorted_indices:\n",
    "            if len(optimal_assignment.get(i, [])) < 2:\n",
    "                optimal_assignment.setdefault(i, []).append(col)\n",
    "                break\n",
    "\n",
    "    unassigned_rows = set(range(num_rows)) - set(optimal_assignment.keys())\n",
    "    \n",
    "    for row in unassigned_rows:\n",
    "        col = np.argmax(graph[row, :])\n",
    "        optimal_assignment[row] = [col]\n",
    "    \n",
    "    return optimal_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c86a3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_chains(timestamps, folder_path, threshold):\n",
    "    current_timestamp = timestamps[0]\n",
    "    current_text_file = os.path.join(folder_path, current_timestamp, 'event_clusters.txt')\n",
    "    current_clusters = read_clusters_from_file(current_text_file)\n",
    "    chains = [{(current_timestamp, cluster_idx + 1)} for cluster_idx, _ in enumerate(current_clusters)]\n",
    "    clusters_map = []\n",
    "    for i in range(len(current_clusters)):\n",
    "        clusters_map.append([i]) \n",
    "    for i in range(1, len(timestamps)):\n",
    "        current_timestamp = timestamps[i - 1]\n",
    "        next_timestamp = timestamps[i]\n",
    "        print(f'{current_timestamp} and {next_timestamp}')\n",
    "        current_text_file = os.path.join(folder_path, current_timestamp, 'event_clusters.txt')\n",
    "        current_hashtag_file = os.path.join(folder_path, current_timestamp, 'event_hashtags.txt')\n",
    "        current_clusters = read_clusters_from_file(current_text_file)\n",
    "        current_hashtag = read_hashtags_from_file(current_hashtag_file)\n",
    "        next_text_file = os.path.join(folder_path, next_timestamp, 'event_clusters.txt')\n",
    "        next_hashtag_file = os.path.join(folder_path, next_timestamp, 'event_hashtags.txt')\n",
    "        next_hashtag = read_hashtags_from_file(next_hashtag_file)\n",
    "        next_clusters = read_clusters_from_file(next_text_file)\n",
    "        graph = create_bipartite_graph(current_clusters, current_hashtag, next_clusters, next_hashtag, threshold)\n",
    "        optimal_assignment = apply_hungarian_method(graph, len(current_clusters), len(next_clusters))\n",
    "        temp_map = [set() for _ in range(len(next_clusters))]\n",
    "\n",
    "        for i in optimal_assignment.keys():\n",
    "            for j in optimal_assignment[i]:\n",
    "                for k in clusters_map[i]:\n",
    "                    temp_map[j].add(k)\n",
    "                    chains[k].add((next_timestamp,j+1))\n",
    "        clusters_map = temp_map\n",
    "    return chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c42872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_dict(folder_path):\n",
    "    clusters_dict = {}\n",
    "\n",
    "    folders = sorted(os.listdir(folder_path)) \n",
    "    for i in range(len(folders)-1):\n",
    "        current_folder = folders[i]\n",
    "        next_folder = folders[i+1]\n",
    "\n",
    "        current_file_path = os.path.join(folder_path, current_folder, 'event_clusters.txt')\n",
    "        next_file_path = os.path.join(folder_path, next_folder, 'event_clusters.txt')\n",
    "\n",
    "        current_clusters = read_clusters_from_file(current_file_path)\n",
    "        next_clusters = read_clusters_from_file(next_file_path)\n",
    "\n",
    "        clusters_dict[current_folder] = current_clusters\n",
    "        clusters_dict[next_folder] = next_clusters\n",
    "\n",
    "    return clusters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e07275a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cluster_chains(cluster_chains, output_folder_path, timestamp_clusters_dict):\n",
    "    output_folder_path = os.path.join(output_folder_path, \"cluster_chains\")\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "    for chain_idx, chain in enumerate(cluster_chains):\n",
    "        chain_folder_path = os.path.join(output_folder_path, f\"cluster_chain_{chain_idx + 1}\")\n",
    "        os.makedirs(chain_folder_path, exist_ok=True)\n",
    "\n",
    "        for index,( timestamp, cluster_idx) in enumerate(chain):\n",
    "            clusters = timestamp_clusters_dict[timestamp]\n",
    "            if 0 <= cluster_idx <= len(clusters):\n",
    "                cluster = clusters[cluster_idx - 1]\n",
    "\n",
    "                cluster_folder_path = os.path.join(chain_folder_path, f\"cluster_{index + 1}\")\n",
    "                os.makedirs(cluster_folder_path, exist_ok=True)\n",
    "                output_file_path = os.path.join(cluster_folder_path, f\"{index+ 1}.txt\")\n",
    "\n",
    "                with open(output_file_path, \"w\") as text_file:\n",
    "                    for tweet_list in cluster:\n",
    "                        for tweet in tweet_list:\n",
    "                            text_file.write(f\"{tweet} \")\n",
    "                        text_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8908c43",
   "metadata": {},
   "source": [
    "# Sentiment analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a027ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_names(folder_path):\n",
    "    entries = os.listdir(folder_path)\n",
    "    folder_names = [entry for entry in entries if os.path.isdir(os.path.join(folder_path, entry))]\n",
    "\n",
    "    return folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24938383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(cluster):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiments = {'positive': 0, 'neutral': 0, 'negative': 0}\n",
    "\n",
    "    for tweet in cluster:\n",
    "        sentiment_scores = analyzer.polarity_scores(tweet)\n",
    "        compound_score = sentiment_scores['compound']\n",
    "\n",
    "        if compound_score >= 0.05:\n",
    "            sentiments['positive'] += 1\n",
    "        elif compound_score > -0.05 and compound_score < 0.05:\n",
    "            sentiments['neutral'] += 1\n",
    "        else:\n",
    "            sentiments['negative'] += 1\n",
    "\n",
    "    return sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "670e2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(sentiments, save_path):\n",
    "    labels = sentiments.keys()\n",
    "    sizes = [sentiments[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=['green', 'yellow', 'red'])\n",
    "    plt.title('Sentiment Analysis')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b8b2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emoji_image(sentiments, save_path):\n",
    "    sentiment = max(sentiments.items(), key = operator.itemgetter(1))[0]\n",
    "    folder = os.getcwd()\n",
    "    emoji_mapping = {\n",
    "        'positive': f'{folder}/positive/emoji.png', \n",
    "        'neutral': f'{folder}/neutral/emoji.png', \n",
    "        'negative': f'{folder}/negative/emoji.png'\n",
    "    }\n",
    "\n",
    "    emoji_path = emoji_mapping.get(sentiment)\n",
    "    emoji_image = Image.open(emoji_path)\n",
    "    emoji_image.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "854fc525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_cloud(cluster, save_path):\n",
    "    text = ' '.join(cluster)\n",
    "    wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, background_color='white').generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a4ed55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_visualize_clusters(cluster_chains_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    for chain_name in os.listdir(cluster_chains_folder):\n",
    "        chain_folder = os.path.join(cluster_chains_folder, chain_name)\n",
    "        output_chain_folder = os.path.join(output_folder, chain_name)\n",
    "        os.makedirs(output_chain_folder, exist_ok=True)\n",
    "\n",
    "        for cluster_name in os.listdir(chain_folder):\n",
    "            cluster_folder_path = os.path.join(chain_folder, cluster_name)\n",
    "\n",
    "            if os.path.isdir(cluster_folder_path):\n",
    "                cluster_tweets = []\n",
    "                for file_name in os.listdir(cluster_folder_path):\n",
    "                    file_path = os.path.join(cluster_folder_path, file_name)\n",
    "\n",
    "                    if os.access(file_path, os.R_OK) and file_name.endswith('.txt'):\n",
    "                        with open(file_path, 'r') as cluster_file:\n",
    "                            cluster_tweets.extend([line.strip() for line in cluster_file])\n",
    "\n",
    "                sentiments = analyze_sentiment(cluster_tweets)\n",
    "\n",
    "                pie_chart_folder = os.path.join(output_chain_folder, cluster_name)\n",
    "                os.makedirs(pie_chart_folder, exist_ok=True)\n",
    "                pie_chart_path = os.path.join(pie_chart_folder, f'{cluster_name}_sentiment_pie_chart.png')\n",
    "                plot_pie_chart(sentiments, pie_chart_path)\n",
    "\n",
    "                emoji_image_folder = os.path.join(output_chain_folder, cluster_name)\n",
    "                os.makedirs(emoji_image_folder, exist_ok=True)\n",
    "                emoji_image_path = os.path.join(emoji_image_folder, f'{cluster_name}_sentiment_emoji.png')\n",
    "                generate_emoji_image(sentiments, emoji_image_path)\n",
    "\n",
    "                word_cloud_folder = os.path.join(output_chain_folder, cluster_name)\n",
    "                os.makedirs(word_cloud_folder, exist_ok=True)\n",
    "                word_cloud_path = os.path.join(word_cloud_folder, f'{cluster_name}_word_cloud.png')\n",
    "                create_word_cloud(cluster_tweets, word_cloud_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9496cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f12e9-3ead-4c1a-b9b6-4acfd86cf0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cef92d-1e03-41d0-b7eb-87ac56d97030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c110f-f2a6-4f79-b7a0-980f8399b72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
