{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd92d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from tweet_prepossesssing_and_clustering import is_english,extract_hashtag,remove_stop_words,lemmatize,tokenize,generate_bow,count_dict,tf_idf_vector,one_hot_encoding_vector,cosine_similarity,jaccard_similarity,dice_similarity,k_means_clustering,most_frequent_words,generate_word_cloud,iterative_clustering\n",
    "from chainning_and_analysis import calculate_silhouette_scores,calculate_cluster_weights,event_clusters_filter,generate_clusters,write_clusters_to_text_and_hashtags,read_clusters_from_file,read_hashtags_from_file,centroid,calculate_similarity,textual_similarity,create_bipartite_graph,apply_hungarian_method,create_cluster_chains,get_folder_names,write_cluster_chains,analyze_sentiment,plot_pie_chart,generate_emoji_image,generate_word_cloud,analyze_and_visualize_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19811b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import emoji\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import operator\n",
    "from textblob import TextBlob\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import emojis\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "962a7498-9b69-4122-82e3-de081f8868d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('omw-1.4')  # Optional, for WordNet lemmatizer's additional language support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88b428e3-b101-4522-8e72-4d2d87bcfca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng', download_dir='C:\\\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c66b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files(input_folder):\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder '{input_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Iterate through all text files in the folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Read the file into a DataFrame\n",
    "            df = pd.read_csv(file_path, names=['Text'])\n",
    "                   \n",
    "            # Handle missing or non-string values\n",
    "            df['Text'] = df['Text'].fillna('').astype(str) # Replace NaNs with empty strings and ensure type is string\n",
    "            \n",
    "            # Filter out non-English tweets\n",
    "            df = df[df['Text'].apply(is_english)]\n",
    "            \n",
    "            # Extract hashtags and tokenize the tweets\n",
    "            extract_hashtag(df)\n",
    "            tokenize(df)\n",
    "            \n",
    "            # Remove stop words and filter out empty tokens\n",
    "            df['filtered_tokens'] = df['Tokenized_Tweets'].apply(remove_stop_words)\n",
    "            df = df[df['filtered_tokens'].apply(lambda x: len(x) > 0)]\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            # Generate Bag of Words and TF-IDF vectors\n",
    "            bag_of_words = generate_bow(df['filtered_tokens'])\n",
    "            vector = tf_idf_vector(df['filtered_tokens'].tolist(), bag_of_words.keys())\n",
    "            vector = np.array(vector)\n",
    "            \n",
    "            # # Perform clustering and evaluate clusters\n",
    "            cluster, no_of_clusters = iterative_clustering(vector, 0.1)\n",
    "            cluster_qualities = calculate_silhouette_scores(np.array(vector), np.array(cluster))\n",
    "            cluster_weights = calculate_cluster_weights(vector, cluster)\n",
    "            event_clusters = event_clusters_filter(cluster_qualities, cluster_weights, 0.7, 6.0)\n",
    "            \n",
    "            # # Generate clusters of tweets and hashtags\n",
    "            event_clusters_tweets, event_clusters_hashtags = generate_clusters(\n",
    "            list(df['filtered_tokens']), cluster, event_clusters, list(df['Hashtag'])\n",
    "             )\n",
    "            no_of_clusters = len(event_clusters)\n",
    "            \n",
    "            if no_of_clusters < 1:\n",
    "                print(f\"No valid clusters found in file: {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # # Extract the most frequent words in each cluster\n",
    "            word_sets = most_frequent_words(event_clusters_tweets, no_of_clusters)\n",
    "            \n",
    "            # # Create output directory for the processed clusters\n",
    "            folder_name = os.path.splitext(filename)[0]\n",
    "            output_folder = os.path.join('event_tweets', folder_name)\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            \n",
    "            # # Write event clusters and hashtags to files\n",
    "            event_cluster_file = os.path.join(output_folder, 'event_clusters.txt')\n",
    "            event_hashtags_file = os.path.join(output_folder, 'event_hashtags.txt')\n",
    "            write_clusters_to_text_and_hashtags(event_clusters_tweets, event_clusters_hashtags, event_cluster_file, event_hashtags_file)\n",
    "            \n",
    "            # # Generate word clouds for each cluster\n",
    "            for i in range(no_of_clusters):\n",
    "                 print(f\"Top words for cluster {i + 1}:\")\n",
    "                 print(sorted(word_sets[i].items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "                 generate_word_cloud(word_sets[i], i, os.path.join(output_folder, f\"img{i}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9d05909-0913-4a7f-b048-ae1621cc333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: batch_1.txt\n",
      "Top words for cluster 1:\n",
      "[('trinbago', 7), ('knight', 7), ('riders', 7), ('time', 4), ('titles', 4), ('tkr', 2), ('champions', 2), ('times', 1), ('tag', 1), ('answer', 1)]\n",
      "Processing file: batch_10.txt\n",
      "No valid clusters found in file: batch_10.txt\n",
      "Processing file: batch_11.txt\n",
      "No valid clusters found in file: batch_11.txt\n",
      "Processing file: batch_12.txt\n",
      "No valid clusters found in file: batch_12.txt\n",
      "Processing file: batch_13.txt\n",
      "No valid clusters found in file: batch_13.txt\n",
      "Processing file: batch_14.txt\n",
      "Top words for cluster 1:\n",
      "[('b', 2)]\n",
      "Processing file: batch_15.txt\n",
      "Top words for cluster 1:\n",
      "[('b', 8), ('join', 4), ('answer', 2), ('ans', 2), ('k', 1), ('l', 1), ('rahul', 1)]\n",
      "Top words for cluster 2:\n",
      "[('fuck', 2), ('fuckkkkkkk', 1)]\n",
      "Top words for cluster 3:\n",
      "[('replacing', 2)]\n",
      "Processing file: batch_16.txt\n",
      "Top words for cluster 1:\n",
      "[('ans', 4), ('join', 4), ('b', 3), ('answer', 1)]\n",
      "Processing file: batch_17.txt\n",
      "Top words for cluster 1:\n",
      "[('v', 9), ('kent', 3), ('worcestershire', 3), ('yorkshire', 2), ('somerset', 2), ('northamptonshire', 2), ('surrey', 2), ('northern', 2), ('diamonds', 2), ('southern', 2)]\n",
      "Processing file: batch_18.txt\n",
      "Top words for cluster 1:\n",
      "[('sussex', 6), ('v', 6), ('middlesex', 6)]\n",
      "Processing file: batch_19.txt\n",
      "Top words for cluster 1:\n",
      "[('sussex', 24), ('v', 24), ('middlesex', 24)]\n",
      "Top words for cluster 2:\n",
      "[('lord', 8), ('thakur', 7), ('generous', 6), ('let', 6), ('rory', 6), ('burn', 6), ('complete', 6), ('fifty', 6), ('dismissing', 6), ('funny', 1)]\n",
      "Processing file: batch_2.txt\n",
      "No valid clusters found in file: batch_2.txt\n",
      "Processing file: batch_20.txt\n",
      "Top words for cluster 1:\n",
      "[('coach', 13), ('waqar', 9), ('misbah', 8), ('resigned', 8), ('head', 7), ('bowling', 5), ('tested', 1), ('positive', 1), ('remain', 1), ('isolation', 1)]\n",
      "Top words for cluster 2:\n",
      "[('icc', 2), ('player', 2), ('month', 2), ('nominee', 2), ('august', 2), ('shaheen', 2), ('shah', 2), ('afridi', 2), ('jasprit', 2), ('bumrah', 2)]\n",
      "Processing file: batch_3.txt\n",
      "Top words for cluster 1:\n",
      "[('football', 8), ('billion', 8), ('fan', 8), ('love', 4), ('cricket', 4)]\n",
      "Processing file: batch_4.txt\n",
      "No valid clusters found in file: batch_4.txt\n",
      "Processing file: batch_5.txt\n",
      "Top words for cluster 1:\n",
      "[('delayed', 2), ('retirement', 2), ('international', 2), ('cricket', 2), ('whisker', 2)]\n",
      "Processing file: batch_6.txt\n",
      "Top words for cluster 1:\n",
      "[('trinbago', 2), ('knight', 2), ('riders', 2), ('titles', 2)]\n",
      "Processing file: batch_7.txt\n",
      "Top words for cluster 1:\n",
      "[('trinbago', 5), ('knight', 5), ('riders', 5), ('titles', 3), ('time', 2), ('ans', 1)]\n",
      "Top words for cluster 2:\n",
      "[('women', 4), ('west', 2), ('indies', 2), ('v', 2), ('south', 2), ('africa', 2)]\n",
      "Processing file: batch_8.txt\n",
      "Top words for cluster 1:\n",
      "[('st', 4), ('kitts', 2), ('nevis', 2), ('patriots', 2), ('v', 2), ('lucia', 2), ('kings', 2)]\n",
      "Top words for cluster 2:\n",
      "[('women', 4), ('west', 2), ('indies', 2), ('v', 2), ('south', 2), ('africa', 2)]\n",
      "Processing file: batch_9.txt\n",
      "No valid clusters found in file: batch_9.txt\n"
     ]
    }
   ],
   "source": [
    "process_text_files('output_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31fbcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(),'event_tweets')\n",
    "timestamps = get_folder_names(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86451781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_1 and batch_14\n",
      "batch_14 and batch_15\n",
      "batch_15 and batch_16\n",
      "batch_16 and batch_17\n",
      "batch_17 and batch_18\n",
      "batch_18 and batch_19\n",
      "batch_19 and batch_20\n",
      "batch_20 and batch_3\n",
      "batch_3 and batch_5\n",
      "batch_5 and batch_6\n",
      "batch_6 and batch_7\n",
      "batch_7 and batch_8\n"
     ]
    }
   ],
   "source": [
    "resulting_chains = create_cluster_chains(timestamps,folder_path,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5675aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_dict(folder_path):\n",
    "    clusters_dict = {}\n",
    "    folders = sorted(os.listdir(folder_path))\n",
    "    for i in range(len(folders)-1):\n",
    "        current_folder = folders[i]\n",
    "        next_folder = folders[i+1]\n",
    "\n",
    "        current_file_path = os.path.join(folder_path, current_folder, 'event_clusters.txt')\n",
    "        next_file_path = os.path.join(folder_path, next_folder, 'event_clusters.txt')\n",
    "\n",
    "        current_clusters = read_clusters_from_file(current_file_path)\n",
    "        next_clusters = read_clusters_from_file(next_file_path)\n",
    "\n",
    "        clusters_dict[current_folder] = current_clusters\n",
    "        clusters_dict[next_folder] = next_clusters\n",
    "\n",
    "    return clusters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de3ff977",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = create_clusters_dict(os.path.join(os.getcwd(),'event_tweets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "614cd095",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cluster_chains(resulting_chains,os.getcwd(),clusters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb638395",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_chains_folder = os.path.join(os.getcwd(),'cluster_chains')\n",
    "output_folder = os.path.join(os.getcwd(),'Sentiment_analysis')\n",
    "analyze_and_visualize_clusters(cluster_chains_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308befa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
